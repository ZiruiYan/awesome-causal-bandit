# Causal Bandits: Learning Good Interventions via Causal Inference

## ðŸ“– Paper Information

**Authors:** Finnian Lattimore, Tor Lattimore, Mark D. Reid  
**Year:** 2016  
**Conference:** Advances in Neural Information Processing Systems (NeurIPS)  
**Paper Link:** [View on NeurIPS](https://proceedings.neurips.cc/paper/2016/hash/b4288d9c0ec0a1841b3b3728321e7088-Abstract.html)

---

## ðŸ“„ Abstract

We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi-arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.

---

## ðŸ”‘ Key Points

- **Novel Framework**: First paper that introduce causal bandit problems
- **Algorithm Development**: Proposes Parallel Bandit Algorithm
- **Theoretical Analysis**: Proves regret bounds for simple regret

---

## ðŸ“š Citation
```bibtex
@article{lattimore2016causal,
  title={Causal bandits: Learning good interventions via causal inference},
  author={Lattimore, Finnian and Lattimore, Tor and Reid, Mark D},
  journal={Proceedings of Advances in Neural Information Processing Systems},
  year={2016}
}
```